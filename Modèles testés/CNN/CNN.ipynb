{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2xd4Siu46c4"
      },
      "source": [
        "## CNN (1D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g54bnDrXcga",
        "outputId": "86813df9-ae6f-4f5c-879e-149f0e260d3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHGtL6su3t67",
        "outputId": "84f15493-3aca-4e75-df34-350db653553c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Train Loss: 1.0098, Train Acc: 0.5005, Val Loss: 0.9275, Val Acc: 0.5998\n",
            "Epoch 2/50, Train Loss: 0.8898, Train Acc: 0.6504, Val Loss: 0.8921, Val Acc: 0.6439\n",
            "Epoch 3/50, Train Loss: 0.8182, Train Acc: 0.7262, Val Loss: 0.8810, Val Acc: 0.6527\n",
            "Epoch 4/50, Train Loss: 0.7703, Train Acc: 0.7792, Val Loss: 0.8813, Val Acc: 0.6568\n",
            "Epoch 5/50, Train Loss: 0.7330, Train Acc: 0.8169, Val Loss: 0.8840, Val Acc: 0.6571\n",
            "Epoch 6/50, Train Loss: 0.7103, Train Acc: 0.8418, Val Loss: 0.8809, Val Acc: 0.6574\n",
            "Epoch 7/50, Train Loss: 0.6942, Train Acc: 0.8569, Val Loss: 0.8746, Val Acc: 0.6673\n",
            "Epoch 8/50, Train Loss: 0.6815, Train Acc: 0.8699, Val Loss: 0.8806, Val Acc: 0.6616\n",
            "Epoch 9/50, Train Loss: 0.6726, Train Acc: 0.8775, Val Loss: 0.8839, Val Acc: 0.6565\n",
            "Epoch 10/50, Train Loss: 0.6681, Train Acc: 0.8833, Val Loss: 0.8778, Val Acc: 0.6673\n",
            "Epoch 11/50, Train Loss: 0.6608, Train Acc: 0.8906, Val Loss: 0.8800, Val Acc: 0.6629\n",
            "Epoch 12/50, Train Loss: 0.6574, Train Acc: 0.8929, Val Loss: 0.8784, Val Acc: 0.6650\n",
            "Epoch 13/50, Train Loss: 0.6538, Train Acc: 0.8976, Val Loss: 0.8753, Val Acc: 0.6674\n",
            "Epoch 14/50, Train Loss: 0.6512, Train Acc: 0.9000, Val Loss: 0.8739, Val Acc: 0.6704\n",
            "Epoch 15/50, Train Loss: 0.6468, Train Acc: 0.9040, Val Loss: 0.8828, Val Acc: 0.6628\n",
            "Epoch 16/50, Train Loss: 0.6453, Train Acc: 0.9061, Val Loss: 0.8777, Val Acc: 0.6683\n",
            "Epoch 17/50, Train Loss: 0.6408, Train Acc: 0.9104, Val Loss: 0.8732, Val Acc: 0.6735\n",
            "Epoch 18/50, Train Loss: 0.6385, Train Acc: 0.9125, Val Loss: 0.8773, Val Acc: 0.6709\n",
            "Epoch 19/50, Train Loss: 0.6399, Train Acc: 0.9111, Val Loss: 0.8776, Val Acc: 0.6684\n",
            "Epoch 20/50, Train Loss: 0.6383, Train Acc: 0.9128, Val Loss: 0.8825, Val Acc: 0.6635\n",
            "Epoch 21/50, Train Loss: 0.6342, Train Acc: 0.9171, Val Loss: 0.8846, Val Acc: 0.6626\n",
            "Epoch 22/50, Train Loss: 0.6335, Train Acc: 0.9176, Val Loss: 0.8826, Val Acc: 0.6650\n",
            "Epoch 23/50, Train Loss: 0.6337, Train Acc: 0.9168, Val Loss: 0.8864, Val Acc: 0.6621\n",
            "Epoch 24/50, Train Loss: 0.6319, Train Acc: 0.9191, Val Loss: 0.8871, Val Acc: 0.6610\n",
            "Epoch 25/50, Train Loss: 0.6349, Train Acc: 0.9159, Val Loss: 0.8768, Val Acc: 0.6721\n",
            "Epoch 26/50, Train Loss: 0.6314, Train Acc: 0.9195, Val Loss: 0.8805, Val Acc: 0.6672\n",
            "Epoch 27/50, Train Loss: 0.6307, Train Acc: 0.9200, Val Loss: 0.8782, Val Acc: 0.6684\n",
            "Epoch 28/50, Train Loss: 0.6308, Train Acc: 0.9203, Val Loss: 0.8833, Val Acc: 0.6659\n",
            "Epoch 29/50, Train Loss: 0.6328, Train Acc: 0.9182, Val Loss: 0.8745, Val Acc: 0.6742\n",
            "Epoch 30/50, Train Loss: 0.6321, Train Acc: 0.9190, Val Loss: 0.8806, Val Acc: 0.6674\n",
            "Epoch 31/50, Train Loss: 0.6314, Train Acc: 0.9194, Val Loss: 0.8735, Val Acc: 0.6755\n",
            "Epoch 32/50, Train Loss: 0.6288, Train Acc: 0.9224, Val Loss: 0.8773, Val Acc: 0.6697\n",
            "Epoch 33/50, Train Loss: 0.6283, Train Acc: 0.9228, Val Loss: 0.8791, Val Acc: 0.6707\n",
            "Epoch 34/50, Train Loss: 0.6270, Train Acc: 0.9244, Val Loss: 0.8701, Val Acc: 0.6786\n",
            "Epoch 35/50, Train Loss: 0.6265, Train Acc: 0.9244, Val Loss: 0.8778, Val Acc: 0.6703\n",
            "Epoch 36/50, Train Loss: 0.6262, Train Acc: 0.9247, Val Loss: 0.8736, Val Acc: 0.6761\n",
            "Epoch 37/50, Train Loss: 0.6245, Train Acc: 0.9266, Val Loss: 0.8773, Val Acc: 0.6701\n",
            "Epoch 38/50, Train Loss: 0.6262, Train Acc: 0.9250, Val Loss: 0.8792, Val Acc: 0.6694\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Charger les données\n",
        "train_texts = df_pandas['text'].values\n",
        "train_labels = df_pandas['sentiment'].values\n",
        "\n",
        "# Paramètres de tokenization\n",
        "max_words = 20000  # Nombre maximum de mots dans le vocabulaire\n",
        "max_len = 100      # Longueur maximale des séquences\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "\n",
        "# Padding des séquences\n",
        "X = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Encodage des labels en format one-hot\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "y = encoder.fit_transform(train_labels.reshape(-1, 1))\n",
        "\n",
        "# Diviser en ensembles de formation, validation et test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convertir les données en tenseurs PyTorch\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.long)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Créer un Dataset PyTorch\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "val_dataset = TextDataset(X_val, y_val)\n",
        "test_dataset = TextDataset(X_test, y_test)\n",
        "\n",
        "# Dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Construire le modèle PyTorch\n",
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_classes):\n",
        "        super(CNN1D, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.conv1d = nn.Conv1d(in_channels=embed_size, out_channels=64, kernel_size=5)\n",
        "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.fc1 = nn.Linear(64, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).permute(0, 2, 1)  # Convertir en [batch_size, embed_size, seq_len]\n",
        "        x = torch.relu(self.conv1d(x))\n",
        "        x = self.global_max_pool(x).squeeze(-1)  # [batch_size, out_channels]\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return torch.softmax(x, dim=1)\n",
        "\n",
        "# Initialiser le modèle\n",
        "model = CNN1D(vocab_size=max_words, embed_size=128, num_classes=3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Entraîner le modèle\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss, train_acc = 0, 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, torch.argmax(y_batch, dim=1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_acc += (outputs.argmax(dim=1) == y_batch.argmax(dim=1)).float().mean().item()\n",
        "\n",
        "        val_loss, val_acc = 0, 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, torch.argmax(y_batch, dim=1))\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_acc += (outputs.argmax(dim=1) == y_batch.argmax(dim=1)).float().mean().item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "              f\"Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
        "              f\"Train Acc: {train_acc/len(train_loader):.4f}, \"\n",
        "              f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
        "              f\"Val Acc: {val_acc/len(val_loader):.4f}\")\n",
        "\n",
        "# Appeler la fonction d'entraînement\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tt6miZFK9B_F"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()  # Met le modèle en mode évaluation\n",
        "    total_loss = 0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, torch.argmax(y_batch, dim=1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Stocker les vraies classes et les prédictions\n",
        "            all_targets.extend(torch.argmax(y_batch, dim=1).cpu().numpy())\n",
        "            all_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    # Calculer la perte moyenne et la précision\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_targets, all_predictions)\n",
        "\n",
        "    # Afficher le rapport détaillé\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_targets, all_predictions))\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCIi6rIj9E69",
        "outputId": "098e4603-5ded-408b-e3d9-fa176ae8110f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.93      6170\n",
            "           1       0.94      0.93      0.93      6131\n",
            "           2       0.94      0.93      0.94      6169\n",
            "\n",
            "    accuracy                           0.93     18470\n",
            "   macro avg       0.93      0.93      0.93     18470\n",
            "weighted avg       0.93      0.93      0.93     18470\n",
            "\n",
            "Performance sur l'ensemble d'entraînement : Perte = 0.6192, Précision = 0.9319\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.60      0.58      1308\n",
            "           1       0.74      0.77      0.75      1314\n",
            "           2       0.71      0.63      0.67      1336\n",
            "\n",
            "    accuracy                           0.67      3958\n",
            "   macro avg       0.67      0.67      0.67      3958\n",
            "weighted avg       0.67      0.67      0.67      3958\n",
            "\n",
            "Performance sur l'ensemble de validation : Perte = 0.8844, Précision = 0.6655\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.63      0.59      1319\n",
            "           1       0.75      0.74      0.75      1364\n",
            "           2       0.74      0.65      0.69      1275\n",
            "\n",
            "    accuracy                           0.67      3958\n",
            "   macro avg       0.68      0.67      0.68      3958\n",
            "weighted avg       0.68      0.67      0.68      3958\n",
            "\n",
            "Performance sur l'ensemble de test : Perte = 0.8752, Précision = 0.6748\n"
          ]
        }
      ],
      "source": [
        "# Évaluer sur l'ensemble d'entraînement\n",
        "train_loss, train_accuracy = evaluate_model(model, train_loader, criterion)\n",
        "print(f\"Performance sur l'ensemble d'entraînement : Perte = {train_loss:.4f}, Précision = {train_accuracy:.4f}\")\n",
        "\n",
        "# Évaluer sur l'ensemble de validation\n",
        "val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n",
        "print(f\"Performance sur l'ensemble de validation : Perte = {val_loss:.4f}, Précision = {val_accuracy:.4f}\")\n",
        "\n",
        "# Évaluer sur l'ensemble de test\n",
        "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
        "print(f\"Performance sur l'ensemble de test : Perte = {test_loss:.4f}, Précision = {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBFGwVjO3t67",
        "outputId": "869dacdf-a0ec-4693-ca06-3ce408a1d0ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: I love this!\n",
            "Predicted Sentiment: 1\n",
            "Text: This is terrible!\n",
            "Predicted Sentiment: 2\n",
            "Text: It’s okay.\n",
            "Predicted Sentiment: 0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Préparation des séquences et du padding\n",
        "new_texts = [\"I love this!\", \"This is terrible!\", \"It’s okay.\"]\n",
        "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "new_padded = pad_sequences(new_sequences, maxlen=max_len)\n",
        "\n",
        "# Conversion en tenseur PyTorch\n",
        "new_padded_tensor = torch.tensor(new_padded, dtype=torch.long)  \n",
        "\n",
        "# Mode évaluation\n",
        "model.eval()\n",
        "\n",
        "# Prédictions\n",
        "with torch.no_grad():  # Désactive le calcul des gradients pour accélérer les prédictions\n",
        "    predictions = model(new_padded_tensor)\n",
        "\n",
        "# Affichage des résultats\n",
        "for text, pred in zip(new_texts, predictions):\n",
        "    predicted_class = torch.argmax(pred).item()  # Trouver la classe prédite\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted Sentiment: {predicted_class}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FGcyrIe5CZx"
      },
      "source": [
        "## CNN 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5xIhCRT5Dlt",
        "outputId": "64472673-179b-4454-b083-db3b26405378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 1.2166, Accuracy: 0.3349\n",
            "Epoch 2/20, Loss: 1.2173, Accuracy: 0.3342\n",
            "Epoch 3/20, Loss: 1.2175, Accuracy: 0.3339\n",
            "Epoch 4/20, Loss: 1.2174, Accuracy: 0.3341\n",
            "Epoch 5/20, Loss: 1.2175, Accuracy: 0.3340\n",
            "Epoch 6/20, Loss: 1.2175, Accuracy: 0.3339\n",
            "Epoch 7/20, Loss: 1.2175, Accuracy: 0.3339\n",
            "Epoch 8/20, Loss: 1.2175, Accuracy: 0.3339\n",
            "Epoch 9/20, Loss: 1.2175, Accuracy: 0.3339\n",
            "Epoch 10/20, Loss: 1.2164, Accuracy: 0.3349\n",
            "Epoch 11/20, Loss: 1.2178, Accuracy: 0.3336\n",
            "Epoch 12/20, Loss: 1.2175, Accuracy: 0.3339\n",
            "Epoch 13/20, Loss: 1.2174, Accuracy: 0.3341\n",
            "Epoch 14/20, Loss: 1.2175, Accuracy: 0.3339\n",
            "Epoch 15/20, Loss: 1.2174, Accuracy: 0.3340\n",
            "Epoch 16/20, Loss: 1.2177, Accuracy: 0.3337\n",
            "Epoch 17/20, Loss: 1.2174, Accuracy: 0.3340\n",
            "Epoch 18/20, Loss: 1.2173, Accuracy: 0.3341\n",
            "Epoch 19/20, Loss: 1.2173, Accuracy: 0.3342\n",
            "Epoch 20/20, Loss: 1.2177, Accuracy: 0.3337\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Charger les données\n",
        "train_texts = df_pandas['text'].values\n",
        "train_labels = df_pandas['sentiment'].values\n",
        "\n",
        "# Paramètres de tokenization\n",
        "max_words = 20000  # Nombre maximum de mots dans le vocabulaire\n",
        "max_len = 100      # Longueur maximale des séquences\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "\n",
        "# Padding des séquences\n",
        "X = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Encodage des labels en format one-hot\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(train_labels.reshape(-1, 1))\n",
        "\n",
        "# Conversion en tenseurs PyTorch\n",
        "X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # Ajout du canal unique\n",
        "X = X.permute(0, 1, 2).unsqueeze(1)  # Réorganise en [batch_size, 1, height, width]\n",
        "y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Diviser en ensembles de formation, validation et test\n",
        "dataset = torch.utils.data.TensorDataset(X, y)\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "class CNN2D(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(CNN2D, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(1, 1))  # Ajustement du noyau à (1, 1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=(1, 1))  # Ajustement du noyau à (1, 1)\n",
        "        self.global_pool = nn.AdaptiveMaxPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(64, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))  # Appliquer ReLU après conv1\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.conv2(x))  # Appliquer ReLU après conv2\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)  # Aplatir les tenseurs pour les couches entièrement connectées\n",
        "        x = self.relu(self.fc1(x))  # ReLU après fc1\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)  # Pas de ReLU ici, car softmax suit\n",
        "        return self.softmax(x)\n",
        "\n",
        "\n",
        "\n",
        "# Initialiser le modèle\n",
        "model = CNN2D(num_classes=3)\n",
        "\n",
        "# Définir l'optimiseur et la fonction de perte\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Entraîner le modèle\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels.argmax(dim=1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels.argmax(dim=1)).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77KzF7SD7jnz",
        "outputId": "a42b8cf5-6891-407d-9dc4-21b4116da936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.2175, Train Accuracy: 0.3339\n",
            "Test Loss: 1.2206, Test Accuracy: 0.3309\n"
          ]
        }
      ],
      "source": [
        "# Évaluation de la performance sur l'ensemble d'entraînement\n",
        "model.eval()  # Passer le modèle en mode évaluation\n",
        "train_loss = 0.0\n",
        "train_correct = 0\n",
        "train_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.argmax(dim=1))\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        train_correct += predicted.eq(labels.argmax(dim=1)).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "\n",
        "train_loss /= len(train_loader.dataset)\n",
        "train_accuracy = train_correct / train_total\n",
        "print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Évaluation de la performance sur l'ensemble de test\n",
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.argmax(dim=1))\n",
        "\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        test_correct += predicted.eq(labels.argmax(dim=1)).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = test_correct / test_total\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE3_CGbA7l1x",
        "outputId": "f4f0f309-5d02-4920-8e92-782b1e439b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: I love this!\n",
            "Predicted Sentiment: 1\n",
            "Text: This is terrible!\n",
            "Predicted Sentiment: 1\n",
            "Text: It’s okay.\n",
            "Predicted Sentiment: 1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Préparation des séquences et du padding\n",
        "new_texts = [\"I love this!\", \"This is terrible!\", \"It’s okay.\"]\n",
        "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "new_padded = pad_sequences(new_sequences, maxlen=max_len)\n",
        "\n",
        "# Conversion en tenseur PyTorch (conversion en float32)\n",
        "new_padded_tensor = torch.tensor(new_padded, dtype=torch.float32)  # Convertir en float32\n",
        "\n",
        "# Ajouter une dimension de canal (formater comme (batch_size, 1, height, width))\n",
        "new_padded_tensor = new_padded_tensor.unsqueeze(1)  # Ajouter la dimension du canal (1, height, width)\n",
        "\n",
        "# La forme attendue par la couche Conv2d est (batch_size, 1, max_len, 1)\n",
        "new_padded_tensor = new_padded_tensor.unsqueeze(3)  # Ajout de la profondeur pour correspondre à (batch_size, 1, max_len, 1)\n",
        "\n",
        "# Mode évaluation\n",
        "model.eval()\n",
        "\n",
        "# Prédictions\n",
        "with torch.no_grad():  # Désactive le calcul des gradients pour accélérer les prédictions\n",
        "    predictions = model(new_padded_tensor)\n",
        "\n",
        "# Affichage des résultats\n",
        "for text, pred in zip(new_texts, predictions):\n",
        "    predicted_class = torch.argmax(pred).item()  # Trouver la classe prédite\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted Sentiment: {predicted_class}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIDZRLeP8O-Y"
      },
      "source": [
        "## CNN 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3o-vQryO5Jl",
        "outputId": "ce9b6c29-1555-4fc5-a851-31ba12927869"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWnNKqqO8Q_C",
        "outputId": "aa144bff-e7df-4c67-d900-81ea2f894158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 2.6014, Accuracy: 0.3621\n",
            "Validation Accuracy: 0.4036\n",
            "Test Accuracy: 0.4051\n",
            "Epoch [2/20], Loss: 1.0723, Accuracy: 0.3979\n",
            "Validation Accuracy: 0.4236\n",
            "Test Accuracy: 0.4377\n",
            "Epoch [3/20], Loss: 1.0578, Accuracy: 0.4044\n",
            "Validation Accuracy: 0.4187\n",
            "Test Accuracy: 0.4388\n",
            "Epoch [4/20], Loss: 1.0544, Accuracy: 0.4099\n",
            "Validation Accuracy: 0.4221\n",
            "Test Accuracy: 0.4331\n",
            "Epoch [5/20], Loss: 1.0493, Accuracy: 0.4116\n",
            "Validation Accuracy: 0.3759\n",
            "Test Accuracy: 0.3903\n",
            "Epoch [6/20], Loss: 1.0472, Accuracy: 0.4116\n",
            "Validation Accuracy: 0.4293\n",
            "Test Accuracy: 0.4487\n",
            "Epoch [7/20], Loss: 1.0511, Accuracy: 0.4089\n",
            "Validation Accuracy: 0.3312\n",
            "Test Accuracy: 0.3452\n",
            "Epoch [8/20], Loss: 1.0933, Accuracy: 0.3482\n",
            "Validation Accuracy: 0.3850\n",
            "Test Accuracy: 0.3945\n",
            "Epoch [9/20], Loss: 1.0775, Accuracy: 0.3676\n",
            "Validation Accuracy: 0.3331\n",
            "Test Accuracy: 0.3471\n",
            "Epoch [10/20], Loss: 1.0935, Accuracy: 0.3404\n",
            "Validation Accuracy: 0.4202\n",
            "Test Accuracy: 0.4399\n",
            "Epoch [11/20], Loss: 1.0636, Accuracy: 0.3900\n",
            "Validation Accuracy: 0.4293\n",
            "Test Accuracy: 0.4479\n",
            "Epoch [12/20], Loss: 1.0501, Accuracy: 0.4068\n",
            "Validation Accuracy: 0.4320\n",
            "Test Accuracy: 0.4627\n",
            "Epoch [13/20], Loss: 1.0429, Accuracy: 0.4108\n",
            "Validation Accuracy: 0.4286\n",
            "Test Accuracy: 0.4528\n",
            "Epoch [14/20], Loss: 1.0402, Accuracy: 0.4154\n",
            "Validation Accuracy: 0.4274\n",
            "Test Accuracy: 0.4460\n",
            "Epoch [15/20], Loss: 1.0365, Accuracy: 0.4187\n",
            "Validation Accuracy: 0.4430\n",
            "Test Accuracy: 0.4638\n",
            "Epoch [16/20], Loss: 1.0351, Accuracy: 0.4204\n",
            "Validation Accuracy: 0.4615\n",
            "Test Accuracy: 0.4778\n",
            "Epoch [17/20], Loss: 1.0319, Accuracy: 0.4226\n",
            "Validation Accuracy: 0.4388\n",
            "Test Accuracy: 0.4524\n",
            "Epoch [18/20], Loss: 1.0343, Accuracy: 0.4201\n",
            "Validation Accuracy: 0.4570\n",
            "Test Accuracy: 0.4710\n",
            "Epoch [19/20], Loss: 1.0320, Accuracy: 0.4211\n",
            "Validation Accuracy: 0.4574\n",
            "Test Accuracy: 0.4778\n",
            "Epoch [20/20], Loss: 1.0317, Accuracy: 0.4233\n",
            "Validation Accuracy: 0.4604\n",
            "Test Accuracy: 0.4729\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Charger les données\n",
        "train_texts = df_pandas['text'].values\n",
        "train_labels = df_pandas['sentiment'].values\n",
        "\n",
        "# Paramètres de tokenization\n",
        "max_words = 20000  # Nombre maximum de mots dans le vocabulaire\n",
        "max_len = 100      # Longueur maximale des séquences\n",
        "embedding_dim = 128  # Taille des vecteurs d'embedding\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "\n",
        "# Padding des séquences\n",
        "X = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Encoder les labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(train_labels)\n",
        "\n",
        "# Convertir en one-hot\n",
        "y = np.eye(3)[y]\n",
        "\n",
        "# Ajouter une dimension pour les embeddings (simuler 3D)\n",
        "X = np.expand_dims(X, axis=-1)  # (num_samples, max_len, 1)\n",
        "X = np.expand_dims(X, axis=-1)  # (num_samples, max_len, 1, 1)\n",
        "\n",
        "# Convertir les données en tensors PyTorch\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "\n",
        "X_tensor = X_tensor.unsqueeze(1)  # Shape devient (batch_size, 1, max_len, 1, 1)\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement, validation et test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Créer des DataLoader pour les lots\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "val_data = TensorDataset(X_val, y_val)\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "# Définir le modèle CNN 3D\n",
        "class CNN3DModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN3DModel, self).__init__()\n",
        "\n",
        "        # Définir les couches du modèle\n",
        "        self.conv1 = nn.Conv3d(1, 32, kernel_size=(3, 1, 1))  \n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=(2, 1, 1))\n",
        "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 1, 1)) \n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 1, 1))\n",
        "\n",
        "        # Calculate the output size dynamically by passing a dummy tensor\n",
        "        self.dummy_input = torch.zeros(1, 1, 100, 1, 1)  # Dummy input tensor\n",
        "        self.conv_output_size = self._get_conv_output(self.dummy_input)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.conv_output_size, 128)  # Updated to match the output of the convolutions\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, 3)  # Sortie pour 3 classes\n",
        "\n",
        "    def _get_conv_output(self, shape):\n",
        "        # Pass the dummy input through the conv layers to calculate the output size\n",
        "        x = self.conv1(shape)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        return int(np.prod(x.size()))  # Flatten the tensor and return the number of features\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passer par les couches\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))  # Appliquer ReLU après Conv3d\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))  # Appliquer ReLU après Conv3d\n",
        "        x = torch.flatten(x, 1)  # Aplatissement avant les couches entièrement connectées\n",
        "        x = torch.relu(self.fc1(x))  # Appliquer ReLU après la couche FC\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)  # Pas d'activation ici, car on l'ajoutera dans la fonction de perte\n",
        "        return x\n",
        "\n",
        "# Initialiser le modèle\n",
        "model = CNN3DModel()\n",
        "\n",
        "# Définir la fonction de perte et l'optimiseur\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Entraînement du modèle avec évaluation sur l'ensemble de test\n",
        "def train(model, train_loader, val_loader, test_loader, epochs=20):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.argmax(dim=1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "\n",
        "        train_accuracy = correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "\n",
        "        val_accuracy = correct / total\n",
        "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Test Evaluation (after every epoch)\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "\n",
        "        test_accuracy = correct / total\n",
        "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Entraîner le modèle avec test évaluation\n",
        "train(model, train_loader, val_loader, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqJjlrrZbGPV"
      },
      "source": [
        "# CNN+LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtUgQDqoPbXN",
        "outputId": "b0644ac1-48f4-4bd1-a983-35fc3be7f47c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 0.9338, Accuracy: 0.5350\n",
            "Validation Accuracy: 0.6071\n",
            "Epoch [2/20], Loss: 0.6919, Accuracy: 0.6977\n",
            "Validation Accuracy: 0.6592\n",
            "Epoch [3/20], Loss: 0.5173, Accuracy: 0.7895\n",
            "Validation Accuracy: 0.6609\n",
            "Epoch [4/20], Loss: 0.3610, Accuracy: 0.8631\n",
            "Validation Accuracy: 0.6493\n",
            "Epoch [5/20], Loss: 0.2394, Accuracy: 0.9134\n",
            "Validation Accuracy: 0.6508\n",
            "Epoch [6/20], Loss: 0.1623, Accuracy: 0.9434\n",
            "Validation Accuracy: 0.6470\n",
            "Epoch [7/20], Loss: 0.1214, Accuracy: 0.9567\n",
            "Validation Accuracy: 0.6458\n",
            "Epoch [8/20], Loss: 0.0895, Accuracy: 0.9680\n",
            "Validation Accuracy: 0.6534\n",
            "Epoch [9/20], Loss: 0.0628, Accuracy: 0.9795\n",
            "Validation Accuracy: 0.6440\n",
            "Epoch [10/20], Loss: 0.0603, Accuracy: 0.9793\n",
            "Validation Accuracy: 0.6536\n",
            "Epoch [11/20], Loss: 0.0574, Accuracy: 0.9798\n",
            "Validation Accuracy: 0.6521\n",
            "Epoch [12/20], Loss: 0.0445, Accuracy: 0.9849\n",
            "Validation Accuracy: 0.6453\n",
            "Epoch [13/20], Loss: 0.0455, Accuracy: 0.9843\n",
            "Validation Accuracy: 0.6488\n",
            "Epoch [14/20], Loss: 0.0369, Accuracy: 0.9873\n",
            "Validation Accuracy: 0.6483\n",
            "Epoch [15/20], Loss: 0.0419, Accuracy: 0.9860\n",
            "Validation Accuracy: 0.6488\n",
            "Epoch [16/20], Loss: 0.0257, Accuracy: 0.9909\n",
            "Validation Accuracy: 0.6556\n",
            "Epoch [17/20], Loss: 0.0261, Accuracy: 0.9905\n",
            "Validation Accuracy: 0.6516\n",
            "Epoch [18/20], Loss: 0.0249, Accuracy: 0.9905\n",
            "Validation Accuracy: 0.6486\n",
            "Epoch [19/20], Loss: 0.0241, Accuracy: 0.9926\n",
            "Validation Accuracy: 0.6549\n",
            "Epoch [20/20], Loss: 0.0282, Accuracy: 0.9900\n",
            "Validation Accuracy: 0.6445\n",
            "Test Accuracy: 0.6508\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Paramètres\n",
        "max_words = 20000  # Taille du vocabulaire\n",
        "max_len = 100      # Longueur maximale des séquences\n",
        "embedding_dim = 128\n",
        "num_classes = 3    # Nombre de classes (par exemple, sentiment)\n",
        "\n",
        "# Charger les données\n",
        "train_texts = df_pandas['text'].values\n",
        "train_labels = df_pandas['sentiment'].values\n",
        "\n",
        "# Tokenization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "X = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Encodage des labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(train_labels)\n",
        "y = np.eye(num_classes)[y]  # One-hot encoding\n",
        "\n",
        "# Convertir en tensors PyTorch\n",
        "X_tensor = torch.tensor(X, dtype=torch.long)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Diviser les données en ensembles de formation, validation et test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_tensor, y_tensor, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Créer des DataLoader pour les lots\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "val_data = TensorDataset(X_val, y_val)\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "# Définir le modèle CNN + LSTM avec PyTorch\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(max_words, embedding_dim)\n",
        "\n",
        "        # Partie CNN\n",
        "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=5)\n",
        "        self.pool = nn.MaxPool1d(4)\n",
        "\n",
        "        # Partie LSTM\n",
        "        self.lstm = nn.LSTM(input_size=64, hidden_size=100, batch_first=True)\n",
        "\n",
        "        # Partie Dense pour la classification\n",
        "        self.fc1 = nn.Linear(100, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(0, 2, 1)  # Changer les dimensions pour Conv1d\n",
        "\n",
        "        # Partie CNN\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.permute(0, 2, 1)  # Revenir aux dimensions pour LSTM\n",
        "\n",
        "        # Partie LSTM\n",
        "        x, (h_n, c_n) = self.lstm(x)\n",
        "        x = h_n[-1]  # Utiliser la dernière sortie du LSTM\n",
        "\n",
        "        # Partie Dense\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # Pas de ReLU ici, car nous appliquons softmax dans la fonction de perte\n",
        "        return x\n",
        "\n",
        "# Initialiser le modèle\n",
        "model = CNNLSTMModel()\n",
        "\n",
        "# Définir la fonction de perte et l'optimiseur\n",
        "criterion = nn.CrossEntropyLoss()  # Utilisation de CrossEntropyLoss pour la classification multi-classes\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Entraînement du modèle\n",
        "def train(model, train_loader, val_loader, epochs=20):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.argmax(dim=1))  # Utilisation de .argmax() pour obtenir l'étiquette de classe\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "\n",
        "        train_accuracy = correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "\n",
        "        val_accuracy = correct / total\n",
        "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Entraîner le modèle\n",
        "train(model, train_loader, val_loader)\n",
        "\n",
        "# Évaluation sur les données de test\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "\n",
        "    test_accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Évaluation sur l'ensemble de test\n",
        "evaluate(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14VGPVtdPbsu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
