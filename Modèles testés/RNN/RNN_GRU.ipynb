{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10280125,"sourceType":"datasetVersion","datasetId":6361388},{"sourceId":10295537,"sourceType":"datasetVersion","datasetId":6372171}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n# Charger les trois fichiers CSV\ndf1 = pd.read_csv('/kaggle/input/analyse-sentiement-kafka-dl-js/train_data_single/part-00000-abd2d631-00fd-4975-a058-ffbd706f7bcb-c000.csv')\ndf2 = pd.read_csv('/kaggle/input/analyse-sentiement-kafka-dl-js/val_data_single/part-00000-8cc101df-e287-47d4-aa3d-6fbe733328ad-c000.csv')\ndf3 = pd.read_csv('/kaggle/input/analyse-sentiement-kafka-dl-js/test_data_single/part-00000-1f0de82b-a6f9-478a-863f-705b0fef0a61-c000.csv')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:20:58.216786Z","iopub.execute_input":"2025-01-18T12:20:58.217015Z","iopub.status.idle":"2025-01-18T12:20:59.201944Z","shell.execute_reply.started":"2025-01-18T12:20:58.216982Z","shell.execute_reply":"2025-01-18T12:20:59.200524Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# importing required libraries \n\nimport pandas as pd\n\n# for pytorch imports\nimport torch\n\n# for functional dependencies like activation function \nimport torch.nn.functional as F\n\n# nn is basic module in Torch which provide different neural network architecture\nimport torch.nn as nn\nimport torch.optim as optim\n\n# CountVectorizer for Bagof words model\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# for padding .. since the LSTM takes input as sequence so it is said that \n#if we have fixed input string computation will be faster and it will improve performance \nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm, tqdm_notebook","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:20:59.202803Z","iopub.execute_input":"2025-01-18T12:20:59.203670Z","iopub.status.idle":"2025-01-18T12:21:02.967946Z","shell.execute_reply.started":"2025-01-18T12:20:59.203639Z","shell.execute_reply":"2025-01-18T12:21:02.967317Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:21:02.968810Z","iopub.execute_input":"2025-01-18T12:21:02.969166Z","iopub.status.idle":"2025-01-18T12:21:03.054593Z","shell.execute_reply.started":"2025-01-18T12:21:02.969144Z","shell.execute_reply":"2025-01-18T12:21:03.053597Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class Sequences(Dataset):\n    def __init__(self, df, max_seq_len):\n        self.max_seq_len = max_seq_len\n        \n        # Remplacer les NaN par une chaîne vide\n        df['text'].fillna('', inplace=True)\n        \n        # Vérifier si des NaN subsistent (facultatif)\n        print(\"Nombre de NaN dans text:\", df['text'].isna().sum())  # Doit afficher 0\n        \n        # Bag of Words (BOW)\n        vectorizer = CountVectorizer(stop_words='english', min_df=0.015)\n        vectorizer.fit(df['text'].tolist())  # Utilisation sécurisée de processed_text\n        \n        # Créer le vocabulaire\n        self.token2idx = vectorizer.vocabulary_\n        self.token2idx['<PAD>'] = max(self.token2idx.values()) + 1\n\n        tokenizer = vectorizer.build_analyzer()\n        self.encode = lambda x: [self.token2idx[token] for token in tokenizer(x)\n                                 if token in self.token2idx]\n        self.pad = lambda x: x + (max_seq_len - len(x)) * [self.token2idx['<PAD>']]\n        \n        # Encoder les séquences\n        sequences = [self.encode(sequence)[:max_seq_len] for sequence in df['text'].tolist()]\n        \n        # Filtrer les séquences vides\n        sequences, self.labels = zip(*[(sequence, label) for sequence, label\n                                       in zip(sequences, df['label'].tolist()) if sequence])\n        self.sequences = [self.pad(sequence) for sequence in sequences]\n\n    def __getitem__(self, i):\n        assert len(self.sequences[i]) == self.max_seq_len\n        return self.sequences[i], self.labels[i]\n\n    def __len__(self):\n        return len(self.sequences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:21:03.056669Z","iopub.execute_input":"2025-01-18T12:21:03.056884Z","iopub.status.idle":"2025-01-18T12:21:03.071913Z","shell.execute_reply.started":"2025-01-18T12:21:03.056866Z","shell.execute_reply":"2025-01-18T12:21:03.071227Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Combiner les DataFrames\nfrom statistics import mode\n\n\ndf = pd.concat([df1, df2, df3], ignore_index=True)\ndf['label'] = df['sentiment']\ndel df['sentiment']\ndel df['processed_words']\ndel df['processed_text']\ndel df['words']\ndf\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:21:03.073249Z","iopub.execute_input":"2025-01-18T12:21:03.073493Z","iopub.status.idle":"2025-01-18T12:21:03.118744Z","shell.execute_reply.started":"2025-01-18T12:21:03.073474Z","shell.execute_reply":"2025-01-18T12:21:03.118083Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                    text  label\n0                                       Go get `em!  lol      0\n1      I like fridays generally, but class is extende...      0\n2      Mc  John  sim posit  3 yr passed  no  softees ...      0\n3                                           What To Say?      0\n4                             from g`s to gents season 1      0\n...                                                  ...    ...\n26381  y do i only have 2 people following me  people...      1\n26382  yay it`s friday... hold on I have to work tomo...      1\n26383  yep, good morning to you all  or night or even...      1\n26384                         yo yo yo! i like ice cream      1\n26385  you can learn a lot from another person.. dont...      1\n\n[26386 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Go get `em!  lol</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I like fridays generally, but class is extende...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Mc  John  sim posit  3 yr passed  no  softees ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What To Say?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>from g`s to gents season 1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>26381</th>\n      <td>y do i only have 2 people following me  people...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26382</th>\n      <td>yay it`s friday... hold on I have to work tomo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26383</th>\n      <td>yep, good morning to you all  or night or even...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26384</th>\n      <td>yo yo yo! i like ice cream</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26385</th>\n      <td>you can learn a lot from another person.. dont...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>26386 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"#Supprimer les nulls\ndf.dropna(inplace=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:21:03.119314Z","iopub.execute_input":"2025-01-18T12:21:03.119534Z","iopub.status.idle":"2025-01-18T12:21:03.126159Z","shell.execute_reply.started":"2025-01-18T12:21:03.119515Z","shell.execute_reply":"2025-01-18T12:21:03.125542Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import re\ndef text_cleaner(tx):\n    \n    text = re.sub(r\"won\\'t\", \"would not\", tx)\n    text = re.sub(r\"im\", \"i am\", tx)\n    text = re.sub(r\"Im\", \"I am\", tx)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    text = re.sub(r\"don\\'t\", \"do not\", text)\n    text = re.sub(r\"shouldn\\'t\", \"should not\", text)\n    text = re.sub(r\"needn\\'t\", \"need not\", text)\n    text = re.sub(r\"hasn\\'t\", \"has not\", text)\n    text = re.sub(r\"haven\\'t\", \"have not\", text)\n    text = re.sub(r\"weren\\'t\", \"were not\", text)\n    text = re.sub(r\"mightn\\'t\", \"might not\", text)\n    text = re.sub(r\"didn\\'t\", \"did not\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub(r'[^a-zA-Z0-9\\!\\?\\.\\@]',' ' , text)\n    text = re.sub(r'[!]+' , '!' , text)\n    text = re.sub(r'[?]+' , '?' , text)\n    text = re.sub(r'[.]+' , '.' , text)\n    text = re.sub(r'[@]+' , '@' , text)\n    text = re.sub(r'unk' , ' ' , text)\n    text = re.sub('\\n', '', text)\n    text = text.lower()\n    text = re.sub(r'[ ]+' , ' ' , text)\n    \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:21:03.126876Z","iopub.execute_input":"2025-01-18T12:21:03.127053Z","iopub.status.idle":"2025-01-18T12:21:03.138654Z","shell.execute_reply.started":"2025-01-18T12:21:03.127037Z","shell.execute_reply":"2025-01-18T12:21:03.137904Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x : text_cleaner(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:21:03.139336Z","iopub.execute_input":"2025-01-18T12:21:03.139577Z","iopub.status.idle":"2025-01-18T12:21:03.976816Z","shell.execute_reply.started":"2025-01-18T12:21:03.139558Z","shell.execute_reply":"2025-01-18T12:21:03.975847Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"dataset = Sequences(df, max_seq_len=128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:21:03.977921Z","iopub.execute_input":"2025-01-18T12:21:03.978228Z","iopub.status.idle":"2025-01-18T12:21:04.603099Z","shell.execute_reply.started":"2025-01-18T12:21:03.978197Z","shell.execute_reply":"2025-01-18T12:21:04.602465Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-4-3eec308b9939>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['text'].fillna('', inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"Nombre de NaN dans text: 0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\n\n# Vérification de la répartition des classes avant équilibrage\nprint(\"Distribution des classes avant équilibrage :\")\n# Convertir dataset.labels en DataFrame pour afficher la distribution\ndf_labels = pd.DataFrame({'label': dataset.labels})\nprint(df_labels['label'].value_counts())\n\n# Extraire les séquences et les labels\nX = dataset.sequences  # Les séquences\ny = dataset.labels  # Les labels\n\n# Diviser les données en ensemble d'entraînement, validation et test (70% - 15% - 15%)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n\n# Appliquer SMOTE uniquement sur l'ensemble d'entraînement\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# Vérification après équilibrage\nprint(\"\\nDistribution des classes après SMOTE (train) :\")\nprint(Counter(y_train_resampled))\nprint(\"\\nDistribution des classes (validation) :\")\nprint(Counter(y_val))\nprint(\"\\nDistribution des classes (test) :\")\nprint(Counter(y_test))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:21:04.603824Z","iopub.execute_input":"2025-01-18T12:21:04.604108Z","iopub.status.idle":"2025-01-18T12:21:06.102880Z","shell.execute_reply.started":"2025-01-18T12:21:04.604080Z","shell.execute_reply":"2025-01-18T12:21:06.102160Z"}},"outputs":[{"name":"stdout","text":"Distribution des classes avant équilibrage :\nlabel\n1    6952\n2    5923\n0    5429\nName: count, dtype: int64\n\nDistribution des classes après SMOTE (train) :\nCounter({1: 4866, 2: 4866, 0: 4866})\n\nDistribution des classes (validation) :\nCounter({1: 1043, 2: 889, 0: 814})\n\nDistribution des classes (test) :\nCounter({1: 1043, 2: 888, 0: 815})\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\nfrom torch.autograd import Variable\nimport torch\n\n# Fonction de collate pour padding des séquences\ndef collate(batch):\n    inputs, labels = zip(*batch)\n    inputs = [torch.tensor(seq) for seq in inputs]\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)  # Padding avec 0\n    labels = torch.tensor(labels)\n    return inputs, labels\n\n# Créer un DataLoader pour l'entraînement, la validation et le test\ntrain_dataset = list(zip(X_train, y_train_resampled))\nval_dataset = list(zip(X_val, y_val))\ntest_dataset = list(zip(X_test, y_test))\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate)\nval_loader = DataLoader(val_dataset, batch_size=128, collate_fn=collate)\ntest_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:21:06.103714Z","iopub.execute_input":"2025-01-18T12:21:06.104138Z","iopub.status.idle":"2025-01-18T12:21:06.116909Z","shell.execute_reply.started":"2025-01-18T12:21:06.104105Z","shell.execute_reply":"2025-01-18T12:21:06.116104Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Fonction d'évaluation\ndef evaluate(model, data_loader, criterion):\n    model.eval()  # Mettre le modèle en mode évaluation\n    total_loss = 0\n    total_accuracy = 0\n    with torch.no_grad():  # Pas de rétropropagation pendant l'évaluation\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(model.device), labels.to(model.device)\n\n            # Passage avant\n            outputs = model(inputs)\n\n            # Calcul de la perte\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n            # Calcul de l'accuracy\n            accuracy = calculate_accuracy(outputs, labels)\n            total_accuracy += accuracy.item()\n\n    avg_loss = total_loss / len(data_loader)\n    avg_accuracy = total_accuracy / len(data_loader)\n    return avg_loss, avg_accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:21:06.118017Z","iopub.execute_input":"2025-01-18T12:21:06.118293Z","iopub.status.idle":"2025-01-18T12:21:06.137093Z","shell.execute_reply.started":"2025-01-18T12:21:06.118273Z","shell.execute_reply":"2025-01-18T12:21:06.136221Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import f1_score\n\n# Charger les embeddings GloVe\ndef load_glove_embeddings(glove_path, vocab):\n    embeddings_index = {}\n    with open(glove_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = vector\n    \n    embedding_dim = len(next(iter(embeddings_index.values())))\n    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n\n    for word, idx in vocab.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[idx] = embedding_vector\n        else:\n            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n    \n    return embedding_matrix, embedding_dim\n\n# Classe du modèle RNN amélioré\nclass RNN(nn.Module):\n    def __init__(self, embedding_layer, hidden_size=256, num_classes=3, device='cpu'):\n        super(RNN, self).__init__()\n        self.device = device\n        self.encoder = nn.Embedding.from_pretrained(embedding_layer, freeze=False)\n        self.rnn = nn.GRU(\n            input_size=embedding_layer.size(1),  # Dimension des embeddings GloVe\n            hidden_size=hidden_size,\n            num_layers=2,  # Couches supplémentaires pour capturer les dépendances\n            bidirectional=True,\n            batch_first=True,\n            dropout=0.3  # Dropout entre les couches GRU)\n        self.layer_norm = nn.LayerNorm(hidden_size * 2)  # Normalisation par couches\n        self.attn = nn.Linear(hidden_size * 2, 1)  # Mécanisme d'attention\n        self.dropout = nn.Dropout(p=0.5)  # Dropout régularisation\n        self.decoder = nn.Linear(hidden_size * 2, num_classes)  # Bidirectionnel x2\n    def forward(self, inputs):\n        embedded = self.encoder(inputs)\n        rnn_output, _ = self.rnn(embedded)\n        rnn_output = self.layer_norm(rnn_output)\n        attn_weights = torch.softmax(self.attn(rnn_output), dim=1)\n        context = torch.sum(attn_weights * rnn_output, dim=1)\n        context = self.dropout(context)\n        output = self.decoder(context) return output\n\n# Fonction pour calculer l'accuracy\ndef calculate_accuracy(predictions, labels):\n    preds = predictions.argmax(dim=1)\n    correct = torch.sum(preds == labels)\n    accuracy = correct.float() / labels.size(0)\n    return accuracy\n\n# Fonction d'évaluation\ndef evaluate(model, data_loader, criterion):\n    model.eval()\n    total_loss = 0\n    total_accuracy = 0\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(model.device), labels.to(model.device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n            accuracy = calculate_accuracy(outputs, labels)\n            total_accuracy += accuracy.item()\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n    \n    avg_loss = total_loss / len(data_loader)\n    avg_accuracy = total_accuracy / len(data_loader)\n    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n    return avg_loss, avg_accuracy, f1\n\n# Charger les embeddings GloVe et définir le vocabulaire\nvocab = dataset.token2idx  # À remplacer par votre vocabulaire\nglove_path = '/kaggle/input/glovetwitter/glove.twitter.27B.200d.txt'\nembedding_matrix, embedding_dim = load_glove_embeddings(glove_path, vocab)\n\n# Convertir les séquences et les labels en Tensors\nX_train_tensor = torch.tensor(X_train_resampled, dtype=torch.long)\ny_train_tensor = torch.tensor(y_train_resampled, dtype=torch.long)\nX_val_tensor = torch.tensor(X_val, dtype=torch.long)\ny_val_tensor = torch.tensor(y_val, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.long)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\n# Créer les DataLoaders\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\nval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Créer le modèle\nmodel = RNN(\n    embedding_layer=torch.FloatTensor(embedding_matrix),\n    hidden_size=256,\n    num_classes=3,\n    device='cuda' if torch.cuda.is_available() else 'cpu'\n)\nmodel = model.to(model.device)\n\n# Fonction de perte et optimiseur\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n\n# Plan de diminution du taux d'apprentissage\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n\n# Entraînement du modèle\nepochs = 30\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    total_accuracy = 0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(model.device), labels.to(model.device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        total_loss += loss.item()\n\n        accuracy = calculate_accuracy(outputs, labels)\n        total_accuracy += accuracy.item()\n\n        loss.backward()\n        optimizer.step()\n    \n    avg_loss = total_loss / len(train_loader)\n    avg_accuracy = total_accuracy / len(train_loader)\n\n    # Validation\n    val_loss, val_accuracy, val_f1 = evaluate(model, val_loader, criterion)\n    scheduler.step(val_loss)\n\n    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_loss:.4f}, Train Accuracy: {avg_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, F1 Score: {val_f1:.4f}\")\n\n# Test final\ntest_loss, test_accuracy, test_f1 = evaluate(model, test_loader, criterion)\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, F1 Score: {test_f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:21:06.139166Z","iopub.execute_input":"2025-01-18T12:21:06.139374Z","iopub.status.idle":"2025-01-18T12:27:43.354343Z","shell.execute_reply.started":"2025-01-18T12:21:06.139356Z","shell.execute_reply":"2025-01-18T12:27:43.353561Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/30], Train Loss: 0.9919, Train Accuracy: 0.5161\nValidation Loss: 0.8867, Validation Accuracy: 0.5742, F1 Score: 0.5793\nEpoch [2/30], Train Loss: 0.9151, Train Accuracy: 0.5645\nValidation Loss: 0.9228, Validation Accuracy: 0.5645, F1 Score: 0.5735\nEpoch [3/30], Train Loss: 0.9030, Train Accuracy: 0.5722\nValidation Loss: 0.8863, Validation Accuracy: 0.5626, F1 Score: 0.5664\nEpoch [4/30], Train Loss: 0.9035, Train Accuracy: 0.5736\nValidation Loss: 0.9041, Validation Accuracy: 0.5677, F1 Score: 0.5765\nEpoch [5/30], Train Loss: 0.9000, Train Accuracy: 0.5721\nValidation Loss: 0.8986, Validation Accuracy: 0.5693, F1 Score: 0.5727\nEpoch [6/30], Train Loss: 0.8982, Train Accuracy: 0.5764\nValidation Loss: 0.8875, Validation Accuracy: 0.5714, F1 Score: 0.5725\nEpoch [7/30], Train Loss: 0.8916, Train Accuracy: 0.5771\nValidation Loss: 0.8948, Validation Accuracy: 0.5775, F1 Score: 0.5843\nEpoch [8/30], Train Loss: 0.8776, Train Accuracy: 0.5852\nValidation Loss: 0.8888, Validation Accuracy: 0.5733, F1 Score: 0.5737\nEpoch [9/30], Train Loss: 0.8764, Train Accuracy: 0.5899\nValidation Loss: 0.8953, Validation Accuracy: 0.5696, F1 Score: 0.5766\nEpoch [10/30], Train Loss: 0.8710, Train Accuracy: 0.5921\nValidation Loss: 0.8904, Validation Accuracy: 0.5779, F1 Score: 0.5838\nEpoch [11/30], Train Loss: 0.8690, Train Accuracy: 0.5917\nValidation Loss: 0.9017, Validation Accuracy: 0.5696, F1 Score: 0.5756\nEpoch [12/30], Train Loss: 0.8613, Train Accuracy: 0.5975\nValidation Loss: 0.8939, Validation Accuracy: 0.5736, F1 Score: 0.5784\nEpoch [13/30], Train Loss: 0.8547, Train Accuracy: 0.6007\nValidation Loss: 0.9028, Validation Accuracy: 0.5743, F1 Score: 0.5798\nEpoch [14/30], Train Loss: 0.8507, Train Accuracy: 0.6038\nValidation Loss: 0.8943, Validation Accuracy: 0.5707, F1 Score: 0.5769\nEpoch [15/30], Train Loss: 0.8487, Train Accuracy: 0.6030\nValidation Loss: 0.8915, Validation Accuracy: 0.5740, F1 Score: 0.5810\nEpoch [16/30], Train Loss: 0.8355, Train Accuracy: 0.6120\nValidation Loss: 0.8999, Validation Accuracy: 0.5722, F1 Score: 0.5794\nEpoch [17/30], Train Loss: 0.8324, Train Accuracy: 0.6148\nValidation Loss: 0.9021, Validation Accuracy: 0.5674, F1 Score: 0.5753\nEpoch [18/30], Train Loss: 0.8277, Train Accuracy: 0.6172\nValidation Loss: 0.9064, Validation Accuracy: 0.5666, F1 Score: 0.5742\nEpoch [19/30], Train Loss: 0.8296, Train Accuracy: 0.6152\nValidation Loss: 0.9130, Validation Accuracy: 0.5660, F1 Score: 0.5737\nEpoch [20/30], Train Loss: 0.8216, Train Accuracy: 0.6186\nValidation Loss: 0.9101, Validation Accuracy: 0.5707, F1 Score: 0.5780\nEpoch [21/30], Train Loss: 0.8199, Train Accuracy: 0.6212\nValidation Loss: 0.9128, Validation Accuracy: 0.5715, F1 Score: 0.5789\nEpoch [22/30], Train Loss: 0.8153, Train Accuracy: 0.6242\nValidation Loss: 0.9142, Validation Accuracy: 0.5656, F1 Score: 0.5724\nEpoch [23/30], Train Loss: 0.8138, Train Accuracy: 0.6255\nValidation Loss: 0.9189, Validation Accuracy: 0.5697, F1 Score: 0.5758\nEpoch [24/30], Train Loss: 0.8089, Train Accuracy: 0.6286\nValidation Loss: 0.9178, Validation Accuracy: 0.5696, F1 Score: 0.5761\nEpoch [25/30], Train Loss: 0.8088, Train Accuracy: 0.6281\nValidation Loss: 0.9210, Validation Accuracy: 0.5653, F1 Score: 0.5725\nEpoch [26/30], Train Loss: 0.8079, Train Accuracy: 0.6290\nValidation Loss: 0.9219, Validation Accuracy: 0.5664, F1 Score: 0.5737\nEpoch [27/30], Train Loss: 0.8059, Train Accuracy: 0.6284\nValidation Loss: 0.9227, Validation Accuracy: 0.5674, F1 Score: 0.5739\nEpoch [28/30], Train Loss: 0.8060, Train Accuracy: 0.6297\nValidation Loss: 0.9241, Validation Accuracy: 0.5649, F1 Score: 0.5721\nEpoch [29/30], Train Loss: 0.8007, Train Accuracy: 0.6328\nValidation Loss: 0.9245, Validation Accuracy: 0.5671, F1 Score: 0.5740\nEpoch [30/30], Train Loss: 0.8028, Train Accuracy: 0.6323\nValidation Loss: 0.9239, Validation Accuracy: 0.5656, F1 Score: 0.5726\nTest Loss: 0.8959, Test Accuracy: 0.5731, F1 Score: 0.5813\n","output_type":"stream"}],"execution_count":13}]}